* Introduction
   Deep Learning is a 'buzz word' that can be heard very often at many places these days. Sources of technical news are constantly coming with new and exciting stories of Deep learning application in almost all walks of life. Several of these even broke through into mainstream media. Big corporation such as Amazon, Apple, Facebook or Google are investing enormous resources into their research. Huge number of start-ups also invested heavily into machine learning technologies and is coming up with all sorts of interesting applications.

   Machine learning is very important and is surrounding us every day without us even knowing it. Traffic cameras are using it to detect registration numbers of cars. Mobile devices that all of us poses in our pockets are capable to identify face of person that is operating them. Companies such as Tesla are developing self-driving cars. Machine learning is all around us.

   One of most prominent machine learning success stories of last several years are without a doubt Convolutional Neural Networks.

   Theoretical principle of Convolutional Neural Networks will be described and several import features that secured their success will be identified

   Part of this work will be also devoted to choosing appropriate machine learning framework from many available open-source solutions.

** History
   History of [[glspl:nn][NN]] can be arguably dated to 1943, when Warren Mcculloch and Walter Pitts devised mathematical model inspired by the biology of brain neuron cell.
   This inspired the invention of Perceptron, created in 1958 by Frank Rosenblatt. Perceptron used very simple model of Neuron that was based on mathematical model of Pitts and Mcculloch. Perceptron model in [picture citation] has multiple inputs $x_1,...,x_n$ that are weighted $w_1,...,w_n$ and summed together. Additionally each neuron has bias $b$ that controls its weight.
   Result is squashed through nonlinear activation function to produce output. Depending on the application, output is bounded between values 0 to 1 or -1 to 1.

   # (citation) http://web.csulb.edu/~cwallis/artificialn/History.htm

   # https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png
   # https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png

   From the beginning Perceptron seemed promising, but it was soon discovered that it has severe limitations. These limitations prevent it from being used to classify even slightly more complicated problems. Most prominent voice of criticism was Marvin Minsky, who published book called Perceptrons [citation]. The criticism was centered on the fact that there was no efficient technique to effectively train Perceptron to solve complex problems. Among others the book contained mathematical proof that Perceptron is unable to solve simple XOR problem. More generally the Perceptron is only capable to solve linearly separable problems. Even though according to Minsky this criticism wasn't malicious, it stifled the interest in Neural Networks for over a decade.

   Interest in [[glspl:nn][NN]] was rejuvenated in the early 80's by the invention of /Back-propagation/ learning algorithm, which enabled the possibility to use multiple Perceptrons to form networks. Perceptrons could also be stacked upon each other to form layers. Neural networks of this type are commonly called [[gls:mlp][MLP]].

   This simple improvement addressed majority of previously raised concerns and enable the application of [[glspl:nn][NN]] in many different technical domains with moderate success.

   In 80's and 90's the interest in [[glspl:nn][NN]] plateaued again, and general research of [[gls:ai][AI]] was more focused on other (typically of less complex nature) machine learning techniques. In the realm of classification problems there were several notable examples (Support Vector Machines, Ensemble methods, Random Forest). [[gls:ai][AI]] research community also developed several other paradigms of [[glspl:nn][NN]] that were similarly inspired by biology of human brain but took different approaches. Most important examples were [[gls:som][SOM]] and /Recursive Networks/ (e.g. /Hopfield Networks/).

   By the year 2000 there was very few research groups that were devoting enough attention to the Neural Networks. There was also certain disdain for Neural Networks in academia and [[gls:ai][AI]] research community. Success of Neural Networks that was promised almost half a century ago was finally encountered around 2006, when the first Networks with large number of hidden layers was successfully trained. This led to mainstream adaption of umbrella term /Deep Learning/ which typically refers to /Deep Neural Networks/. The word deep signifies that networks have large number of hidden layers. The key theoretical insight was that to learn complicated functions that can represent high-level abstractions (e.g. vision, language understanding etc.) there is a need for deep architecture.
  [[glspl:nn][NN]] in the times before Deep Learning typically had only 1 or 2 hidden layers. These are today often called shallow networks. Typical Deep Networks can have number of hidden layers in order of tens, but in some cases even hundreds [citation]
   # https://www.microsoft.com/en-us/research/publication/foundations-and-trends-in-signal-processing-deep-learning-methods-and-applications-now-publishers/
   Even though that progress of Neural Network into direction of structures with high number of hidden layers was obvious, its training was unsolved technical problem for very long time. There were basically 3 reasons why this breakthrough didn't come sooner.
   1. There were no learning algorithms allowing number of hidden layers to scale.
   2. There wasn't enough of labeled data to effectively train the Neural Network.
   3. The computational hardware wasn't powerful enough to train sufficiently large and complex networks
   First problem was tackled by invention of [[glspl:cnn][CNN]] [citation].
   # LeCunn 1989
   Second problem was solved simply when there was more data available. This was mainly achieved thanks to effort of large companies (Google, Facebook, YouTube, etc.) but also with help of large community of professionals and hobbyists in data sciences.
   Both innovation in computational hardware and improvement of training methods were needed to solve the third problem. One of the technical breakthroughs was utilization of [[gls:gpgpu][GPGPU]]. Thanks to the fact that training process of Neural Networks is typically large number of simple consequent computations, there is a great potential for parallelization (this is specifically true in case of [[glspl:cnn][CNN]]).
