** History
   History of /Neural Networks/ (*NN*) can be arguably dated to 1943, when Warren Mcculloch and Walter Pitts devised mathematical model inspired by the biology of brain neuron cell.
   This inspired the invention of Perceptron, created in 1958 by Frank Rosenblatt. Perceptron used very simple model of Neuron that was based on mathematical model of Pitts and Mcculloch. Perceptron model in [picture citation] has multiple inputs $x_1,...,x_n$ that are weighted $w_1,...,w_n$ and summed together. Additionally each neuron has bias $b$ that controls its weight.
   Result is squashed through nonlinear activation function to produce output. Depending on the application, output is bounded between values 0 to 1 or -1 to 1.

   # (citation) http://web.csulb.edu/~cwallis/artificialn/History.htm

   # https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png
   # https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png

   From the beginning Perceptron seemed promising, but it was soon discovered that it has severe limitations. These limitations prevent it from being used to classify even slightly more complicated problems. Most prominent voice of criticism was Marvin Minsky, who published book called Perceptrons [citation]. The criticism was centered on the fact that there was no efficient technique to effectively train Perceptron to solve complex problems. Among others the book contained mathematical proof that Perceptron is unable to solve simple XOR problem. More generally the Perceptron is only capable to solve linearly separable problems. Even though according to Minsky this criticism wasn't malicious, it stifled the interest in Neural Networks for over a decade.

   Interest in NN was rejuvenated in the early 80's by the invention of /Back-propagation/ learning algorithm, which enabled the possibility to use multiple Perceptrons to form networks. Perceptrons could also be stacked upon each other to form layers. Neural networks of this type are commonly called /Multilayer Perceptron/.

   This simple improvement addressed majority of previously raised concerns and enable the application of NN in many different technical domains with moderate success.

   In 80's and 90's the interest in NN plateaued again, and general research of /Artificial Intelligence/ (*AI*) was more focused on other (typically of less complex nature) machine learning techniques. In the realm of classification problems there were several notable examples (Support Vector Machines, Ensemble methods, Random Forest). AI research community also developed several other paradigms of /Neural Networks/ that were similarly inspired by biology of human brain but took different approaches. Most important examples were /Self Organizing Maps/ (SOM) and /Recursive Networks/ (e.g. /Hopfield Networks/).

   By the year 2000 there was very few research groups that were devoting enough attention to the Neural Networks. There was also certain disdain for Neural Networks in academia and *AI* research community. Success of Neural Networks that was promised almost half a century ago was finally encountered around 2006, when the first Networks with large number of hidden layers was successfully trained. This led to mainstream adaption of umbrella term /Deep Learning/ which typically refers to /Deep Neural Networks/. The word deep signifies that networks have large number of hidden layers. The key theoretical insight was that to learn complicated functions that can represent high-level abstractions (e.g. vision, language understanding etc.) there is a need for deep architecture.
   *NN* in the times before Deep Learning typically had only 1 or 2 hidden layers. These are today often called shallow networks. Typical Deep Networks can have number of hidden layers in order of tens, but in some cases even hundreds [citation]
   # https://www.microsoft.com/en-us/research/publication/foundations-and-trends-in-signal-processing-deep-learning-methods-and-applications-now-publishers/
   Even though that progress of Neural Network into direction of structures with high number of hidden layers was obvious, its training was unsolved technical problem for very long time. There were basically 3 reasons why this breakthrough didn't come sooner.
   1. There were no learning algorithms allowing number of hidden layers to scale.
   2. There wasn't enough of labeled data to effectively train the Neural Network.
   3. The computational hardware wasn't powerful enough to train sufficiently large and complex networks
   First problem was tackled by invention of Convolutional Neural Networks [citation].
   # LeCunn 1989
   Second problem was solved simply when there was more data available. This was mainly achieved thanks to effort of large companies (Google, Facebook, YouTube, etc.) but also with help of large community of professionals and hobbyists in data sciences.
   Both innovation in computational hardware and improvement of training methods were needed to solve the third problem. One of the technical breakthroughs was utilization of /General-Purpose/ computing on /Graphics Processing Units/ (GPGPU). Thanks to the fact that training process of Neural Networks is typically large number of simple consequent computations, there is a great potential for parallelization (this is specifically true in case of /Convolutional Neural Networks/).
