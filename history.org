#+TITLE:       History of Artificial Neural Networks
#+AUTHOR:      Ondrej Zapletal
#+EMAIL:       ondrej.zapletal.oz@gmail.com
#+OPTIONS:     H:2 num:t toc:t \n:nil ::t |:t ^:t f:t tex:t
* History
History of /Artificial Neural Network/
  Evolution of /Artificial Neural Network/ concept began with Warren Mcculloch and Walter Pitts in 1943, when they devised mathematical model inspired by the biology of Neuron cell. This inspired the invention of /Perceptron/, created in 1958 by Frank Rosenblatt. /Perceptron/ used very simple model of /Artificial Neuron/ that was modeled based on biological /Neuron/. This model consist of variable number of weighted inputs which summed and squashed through nonlinear function to produce output (typically bounded between values $(0,1)$ or $(-1,1)$).

  From the beginning /Perceptron/ seamed promising, but it was soon discovered that it has severe limitations. These limitations prevent it from being used to classify even slightly more complicated problems. Most prominent voice of criticism was Marvin Minsky, who published book called Perceptrons. The criticism was centered on the fact that there was no efficient way how to effectively train /Perceptron/ to solve complex problems. Among others the book contained mathematical proof that /Perceptron/ is unable to solve simple XOR problem (in other words, it can only solve linearly separable problems). Even though according to Minsky this criticism wasn't malicious, it stifled the interest in Neural Networks for over a decade.

  Interest in NN was rejuvenated in the early 80 by the invention of /Backpropagation/ learning algorithm, which enabled the possibility to use multiple Perceptrons to form networks. Perceptrons could also be stacked upon each other to form layers. Neural networks of this type are commonly called /Multilayer Perceptron/.

  This simple improvement addressed majority of previously raised concerns and enable the application of NN in many different technical domains with moderate success.

  In 80 and 90 the interest in NN plateaued again, and general research of AI was more focused on other (typically of less complex nature) machine learning techniques. In the realm of classification problems it were notably /Support Vector Machines/ (SVM), /Ensemble methods/ and /Decision trees/ (/Random Forest/). AI research community also developed several other paradigms of /Neural Networks/ that were similarly inspired by biology of human brain but took different approach. Notably /Self Organizing Maps/ (SOM) and /Recursive Networks/ (e.g. /Hopfields Networks/).

  By the year 2000 there was very few research groups that were devoting enough attention to the Neural Networks. There was also certain disdain for Neural Networks in academia and AI research community. Success of Neural Networks that was promised almost half a century ago was finally encountered around 2006, when the first Networks with large number of hidden layers was successfully trained. This lead to mainstream adaption of umbrella term /Deep Learning/ (specifically /Deep Neural Network/). The word deep signifies that depth (number of hidden layers) of these networks is large. The key theoretical insight was that to learn complicated functions that can represent high-level abstractions (e.g. vision, language understanding etc.) there is a need for deep architecture (deep in this context can mean number of layers in order of $10^1$ to $10^3$). Even though that progress of Neural Network into direction of structures with high number of hidden layers was obvious, its training was unsolved technical problem. There were basically two reason why this breakthrough didn't come sooner. Firstly there wasn't enough data to effectively train the /Neural Network/. Secondly the computational hardware until then wasn't powerful enough to train sufficiently large and complex networks and also the training methods used in /Multilayer Perceptron/ weren't sufficient for deep networks.

  First problem was solved simply by availability of more data, which were obtained mainly thanks to effort of large companies (Google, Facebook, Youtube, etc.) but also with effort of large community of professionals and hobbyists of data science.

  Both innovation in computational hardware and improvement of training methods were needed to solve the second problem. One of the technical breakthroughs was utilization of /General-Purpose computing on Graphics Processing Units/ (/GPGPU/). Thanks to the fact that training process of Neural Networks is typically large number of simple consequent computations, there is a great potential for parallelization (this is specifically true in case of /Convolutional neural networks/).
