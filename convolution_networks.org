* Convolutional Networks
Convolutional networks are arguably most successful biologically inspired models in artificial intelligence. Even though they were guided by many different fields, the core design principles were drawn from neuroscience.

# add description of cat experiment page 364

  First documented use of Convolutional Network can be found in [LeCunn 1989]. LeCunn and other were using
Convolutional Neural Networks were originally deployed in image recognition problems. And were inspired by biology of human visual(setq org-highlight-latex-and-related '(latex script entities)) cortex.

# As it stands we will presume that convolutional layer is working with rectangular input data. We will strip away the complexity introduced by working with colored images (i.e input ads additional dimension for color channels) Apart from this we will be dealing only with rectangular inputs (e.g images) and forget the fact that Convolutional networks can be also trained to use one dimensional input (e.g sound) or three dimensional (e.g mri images)

Each convolutional layer requires rectangular input and produces rectangular output

** Convolutional Neural Networks
   /Convolutional Neural Network/ (/CNN/) is specialized type of /Artificial Neural Network/ that was originally used in image processing applications. Where was taken advantage of their two dimensional character. Since then they were also very successfully employed in natural language and video processing.
   Invention of CNNs was inspired by biology of human visual cortex. In human eye light falls on the cornea where photosensitive cells fire signal for individual neurons that are connected to these cells. Neurons in visual cortex are typically connected to only few photosensitive cells which translates into the fact that neurons react only to small portion of observed scene.
   Therefore /Convolutional Neural Networks/ typically aren't fully connected. This has positive effect on computational complexity of network training. Usually complexity of training is rising proportionally (and not exponentially opposed to classical /Fully Connected Neural Networks/) to number of inputs.

*** Structure of /CNN/

    Structure of Convolutional networks is typically composed of three different types of layers. Layer can be of /Convolutional/, /Pooling/ and /Fully-connected/ type.
Even though there is no strict rule enforcing this, it custom to Network layers can pretty much arbitrarily combine these three types of layers (with exception of Fully-Connected layers, which always have to come last). Each type of layer has different rules for signal forward and error backward propagation.

**** Convolutional layer

     As the name suggests this layer employs convolution operation. Input into this layer is simply called /input/. Convolution operation is performed on /input/ with specific filter, that is called /kernel/. Output of convolution operation is typically called /feature map/. Input into Convolutional layer is either image (in case of first network layer) or /feature map/ from previous layer. /Kernel/ is typically of square shape and its width can range from 3 to N pixels (typically /3/, /5/ or /7/). /Feature map/ is created by convolution of /kernel/ over each specified element of /input/.

     Depending on the size of /kernel/ and layer's /padding/ preferences the process of convolution can produce /feature map/ of different size than /input/. When the size of output should be preserved it is necessary to employ /zero padding/ on the edges of /input/. /Zero padding/ in this case has to add necessary amount of zero elements around the edges of input. This amount is determined by

     $$l = ((h - 1) / 2)$$

where /h/ is width of used kernel. In opposite case the /feature map/ is reduced by the $2*l$. Decreasing of the /feature map/ can be in some cases desirable.

Reduction of /feature map/ can go even further in case of use of /stride/. Application of /stride/ specifies by how many input points is traversed when moving to neighboring position in each step. When the /stride/ is /1/, /kernel/ is moved by /1/ on each step and the resulting size of /feature map/ is not affected.

     Each Convolutional layer is typically composition of several different /kernels/. In other words output of this layer is tensor containing /feature map/ for each used kernel. Each of these  is designed to underline different features of input image. In the first layers these features are typically edges. In following layers the higher the layer the more complex features are captured.

     The fact that each convolution on /input/ is using one /kernel/
# not to be confuse with use of multiple /kernels/ in previous paragraph
 basically means that all connections between two neighboring layers are sharing the same weights. This might not be sufficient in some applications and therefore it is possible to use two other types of connections. /Locally connected/ which basically means that applied /kernel/ is of the same size as the /input/ and /tiled convolution/ which means alternation of more than one set of weights on entire /input/.

 Each convolutional layer have non-linearity on its output
Sometimes this is called the /detector/ stage.

**** Pooling layer

     This layer is used to down sample size of the /input/ layer. Output from this layer is created by various combination of /input/. Max-pooling is one of the more prevalent examples. The input is divided into equal rectangular sub-elements of size larger than 1. Output from each sub-element is then selected as maximal value of its individual elements. This decreases the size of output layer while preserving information contained in input layer and effectively compresses contained information.
# describe different types of pooling:
# 	Average
# 	Maximum
# 	Linear combination

**** Fully-Connected layer


     Fully-Connected layer is typical layer from classical /Feed-forward fully connected Neural Network/ and it is always located on the end of the layer stack.

Utility of multiple fully connected layers at the end of the CNN stack is in some literature questioned.
In other words it is never followed by another Convolutional layer.


Convolution
** Training
# For this moment we will limit are self to explanation of what stages are typically encountered during Network learning.
   Training process of /CNN/ is analogues to /FCNN/ in that both are using /Feed-forward/ and _Backward error propagation_ phases.

# is Feed-Forward actual term used in literature ???

Situation with /CNN/ is more complicated because network is composed of different types of layers and therefore training must accommodate for variability between different layers and also the individual convolution layers are sharing weights across all neurons in each layer.

First phase is the /Feed-Forward/ pass where the signal is propagated from inputs of the /CNN/ to its output. In the last layer the output is compared with desired values by /Error function E/ and error is estimated.
Secondly the Error is propagated backwards through the network and weights for individual layer are updated based on the sum of individual error connections are

Feed /Gradient Decent Methods/.

** Feed-forward pass
*** Convolution Layer

    Computation of convolution output $x_{ij}^{(l)}$ for point /i/, /j/ is defined as

$$ x_{ij}^{(l)}=\sum_{a=0}^{m-1}\sum_{b=0}^{m-1}\omega_{ab}y_{(i+a)(j+b)}^{(l-1)}$$

where $\omega_{ab}$ are weight representing /kernel/,

$y_{(i+a)(j+b)}^{(l-1)}$ is output of previous layer

/m/ and /n/ represent dimensions of the /kernel/.

In some cases convolution layer can also apply non-linearity on the output convolution operation $x_{ij}^{(l)}$

$$ y_{ij}^{(l)}=\sigma(x_{ij}^{(l)})$$

where $\sigma$ represents this non-linear function.

*** Pooling layer (Max-Pooling)
Feed forward operation of pooling layer is generally very simple and it constitutes in pooling of multiple inputs into single output. Ratio is typically /4/ to /1/, which means that input matrix is divided into sub-matrices of size /2x2/ and each of these produces one output. In case of /Max-Pooling/ is output computed as the highest value of inputs (hence the name /Max-Pooling/).

** Backward propagation
*** Convolution Layer
Backpropagation

$$
  \frac{\partial E}         {\partial \omega_{ab} }
  =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m}
  \frac{\partial E}          {\partial x_{ij}^{(l)}  }
  \frac{\partial x_{ij}^{(l)}} {\partial \omega_{ab} }
  =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m}
  \frac{\partial E}          {\partial x_{ij}^{(l)}  }
  y_{(i+a)(j+b)}^{(l-1)}
$$

$$
  \frac{\partial E}           {\partial x_{ij}^{(l)}  }
  =\frac{\partial E}          {\partial y_{ij}^{(l)}  }
   \frac{\partial y_{ij}^{(l)}} {\partial x_{ij}^{(l)}  }
  =\frac{\partial E}          {\partial y_{ij}^{(l)}  }
   \frac{\partial }           {\partial x_{ij}^{(l)}  }
  \left( \sigma\left(x_{ij}^{(l)}\right) \right)
  =\frac{\partial E}          {\partial y_{ij}^{(l)}  }
  \sigma' \left( x_{ij}^{(l)} \right)
$$

$$
  \frac{\partial E}         {\partial y_{ij}^{(l-1)} }
  =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1}
  \frac{\partial E}          {\partial x_{(i-a)(j-b)}^{(l)} }
  \frac{\partial x_{(i-a)(j-b)}^{(l)} }          {\partial  y_{ij}^{(l-1)}}
  =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1}
  \frac{\partial E}          {\partial x_{(i-a)(j-b)}^{(l)} }
  \omega_{ab}
$$

*** Pooling layer
    # As it was mentioned in section for feed forward pass, there is no learning going on in pooling layer. Since layer only down-samples output this applies in error propagation as well.
    # Error is propageted backwards depending on how it was propagated forward. In case for max pooling type of layer the error is propagated only to the unit with maximal output in feed forward phase (in other words to the winner of pooling operation). As result of this the error is propagated very sparsely.

# In case of different pooling method it is adjusted accordingly (i.e for average pooling the error is propagated according to contribution of individual neurons).
 means that
Back propagation of Error in pooling layer is simply redistribution
